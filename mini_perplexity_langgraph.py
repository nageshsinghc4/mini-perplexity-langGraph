# -*- coding: utf-8 -*-
"""mini_perplexity_langGraph.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QkEFbYYthE3Izhz_Pt66ayZa8Xh4x6vE
"""

pip install langgraph langchain-openai langchain-community ddgs beautifulsoup4 requests python-dotenv tiktoken langchain-groq

# Import necessary libraries
import warnings
warnings.filterwarnings("ignore")
import os
from dotenv import load_dotenv
from typing import Dict, List, Any, TypedDict
import requests
from bs4 import BeautifulSoup
import json
from ddgs import DDGS  # Changed from duckduckgo_search to ddgs

# LangChain imports
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END

"""#### State Management with TypedDict"""

# Define the state structure using TypedDict
class ResearchState(TypedDict):
    """State for the research assistant"""
    question: str
    search_results: List[Dict[str, Any]]
    extracted_content: List[Dict[str, Any]]
    synthesized_answer: str
    final_answer: str
    sources: List[Dict[str, str]]
    current_step: str

"""#### Defining the Nodes

**1. Search Node**

This node is to perform web search. We use duckduckgo API which is a free API for web-search.
"""

def search_node(state: ResearchState) -> ResearchState:
        """Node for performing web search"""
        print("ğŸ” Searching the web...")
        state["current_step"] = "searching"

        try:
            # Use DuckDuckGo for search (free alternative)
            ddgs = DDGS()
            results = list(ddgs.text(
                state["question"],
                max_results=5,
                region='wt-wt',
                safesearch='moderate'
            ))

            state["search_results"] = results
            print(f"âœ… Found {len(results)} search results")

        except Exception as e:
            print(f"âŒ Search error: {e}")
            state["search_results"] = []

        return state

try:
    ddgs_instance = DDGS()

    # Test with a very general query to ensure basic functionality
    test_query = "general knowledge facts"
    test_results = list(ddgs_instance.text(test_query, max_results=5))
    print(f"Test query ('{test_query}') results: {len(test_results)} results found.")
    if test_results:
        print(f"First test result: {test_results[0]}")
    else:
        print("Test query returned no results. There might be an issue with DDGS service or network connection.")

    # Original query
    original_query = "What's the name of the latest model from OpenAI?"
    result = list(ddgs_instance.text(original_query, max_results=5))
    print(f"Original query ('{original_query}') results: {len(result)} results found.")
    if result:
        print(f"First original query result: {result[0]}")
    else:
        print("Original query returned no results. This might be due to the specific search term or a transient issue.")

except Exception as e:
    print(f"An error occurred during DuckDuckGo search: {e}")
    result = [] # Ensure 'result' is defined even if an error occurs

"""**2. Content Extraction Node**

From the given URLS, this node extracts the content using BeautifulSoup library.
"""

def extract_content_from_url(url, session):
    """Extract main text content from a given URL."""
    try:
        response = session.get(url, timeout=8)
        soup = BeautifulSoup(response.content, 'html.parser')

        # Remove unwanted tags
        for element in soup(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()

        # Try to find main content area
        main_content = (soup.find('main') or
                        soup.find('article') or
                        soup.find('div', class_=lambda x: x and any(
                            keyword in str(x).lower()
                            for keyword in ['content', 'main', 'post', 'article']
                        )))

        if main_content:
            text = main_content.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)

        # Clean text
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)

        return text[:2500]

    except Exception as e:
        return f"Error extracting content: {str(e)}"


def content_extraction_node(state):
    """Extract article content from search results."""
    print("ğŸ“„ Extracting content from search results...")
    state["current_step"] = "extracting_content"

    session = requests.Session()
    session.headers.update({
        'User-Agent': (
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
            'AppleWebKit/537.36 (KHTML, like Gecko) '
            'Chrome/91.0.4472.124 Safari/537.36'
        )
    })

    extracted_content = []
    sources = []

    for i, result in enumerate(state["search_results"][:3]):  # Top 3
        title = result.get('title', 'No title')
        url = result.get('href', '')
        snippet = result.get('body', '')

        print(f"  Extracting from: {title[:50]}...")
        content = extract_content_from_url(url, session)

        extracted_content.append({
            'title': title,
            'url': url,
            'snippet': snippet,
            'content': content,
            'source_id': i + 1
        })

        sources.append({
            'title': title,
            'url': url,
            'snippet': snippet
        })

    state["extracted_content"] = extracted_content
    state["sources"] = sources
    print(f"âœ… Extracted content from {len(extracted_content)} sources")

    return state

"""**3. Synthesis Node**

This node generates answer based on the extracted content using LLM (Large Language Model). You can use any LLM.
"""

def synthesis_node(state):
    """Generate a synthesized answer from extracted content."""
    print("ğŸ§  Synthesizing information and generating answer...")
    state["current_step"] = "synthesizing"

    # Initialize Groq LLM
    llm = ChatGroq(
        api_key='***',
        model='qwen/qwen3-32b',
        temperature=0
    )

    prompt = ChatPromptTemplate.from_messages([
        ("system", """You are a helpful research assistant that provides accurate, well-cited answers based on search results.

Guidelines:
1. Provide a comprehensive answer that directly addresses the question
2. Cite sources using [1], [2], etc.
3. Be factual and objective
4. Include relevant details and context
5. If information is conflicting, mention this
6. If information is insufficient, acknowledge limitations

Structure:
- Start with a direct answer
- Provide detailed explanation with citations
- Include key facts and figures
- End with a concise summary"""),
        ("human", """Question: {question}

Search Results:
{context}

Please provide a comprehensive answer based on the above sources. Cite sources using [1], [2], etc.""")
    ])

    # Build context
    context = ""
    for item in state["extracted_content"]:
        context += f"\nSource [{item['source_id']}]:\n"
        context += f"Title: {item['title']}\n"
        context += f"URL: {item['url']}\n"
        context += f"Snippet: {item['snippet']}\n"
        context += f"Content: {item['content'][:800]}...\n{'-'*50}"

    try:
        chain = prompt | llm | StrOutputParser()
        response = chain.invoke({
            "question": state["question"],
            "context": context
        })

        state["synthesized_answer"] = response
        print("âœ… Answer generated successfully")

    except Exception as e:
        state["synthesized_answer"] = f"Error generating answer: {str(e)}"
        print(f"âŒ Error in synthesis: {e}")

    return state

"""**4. Refinement Node**

This node is again an LLM API call which refines the answer generated in the previous node.

"""

def refinement_node(state):
    """Refine the synthesized answer for clarity and conciseness."""
    print("âœ¨ Refining answer...")
    state["current_step"] = "refining"

    # Initialize Groq LLM
    llm = ChatGroq(
        api_key='***',
        model='qwen/qwen3-32b',
        temperature=0
    )

    refinement_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an editor that refines answers to be more concise, clear, and well-structured.

Requirements:
1. Maintain all citations [1], [2], etc.
2. Improve flow and readability
3. Remove redundant information
4. Ensure factual accuracy
5. Make the answer engaging"""),
        ("human", """Please refine this answer:

QUESTION: {question}

ORIGINAL ANSWER:
{original_answer}

Please provide the refined version:""")
    ])

    try:
        chain = refinement_prompt | llm | StrOutputParser()
        response = chain.invoke({
            "original_answer": state["synthesized_answer"],
            "question": state["question"]
        })

        state["final_answer"] = response
        print("âœ… Answer refined successfully")

    except Exception as e:
        state["final_answer"] = state["synthesized_answer"]
        print(f"âš ï¸ Refinement failed, using original answer: {e}")

    return state

"""#### Building the LangGraph

"""

def create_research_assistant():
    """Create the research assistant graph"""

    # Initialize graph
    workflow = StateGraph(ResearchState)

    workflow.add_node("search", search_node)
    workflow.add_node("extract_content", content_extraction_node)
    workflow.add_node("synthesize", synthesis_node)
    workflow.add_node("refine", refinement_node)

    # Define edges
    workflow.set_entry_point("search")
    workflow.add_edge("search", "extract_content")
    workflow.add_edge("extract_content", "synthesize")
    workflow.add_edge("synthesize", "refine")
    workflow.add_edge("refine", END)

    # Compile graph
    app = workflow.compile()

    return app

# Create the research assistant
research_assistant = create_research_assistant()
print("âœ… Research assistant created successfully!")

# === Display the Graph ===
from IPython.display import Image, display
display(Image(research_assistant.get_graph().draw_mermaid_png()))

# Utility functions for running the assistant

def run_research_assistant(question: str) -> Dict[str, Any]:
    """Run the research assistant on a question"""

    # Initialize state
    initial_state = ResearchState({
        "question": question,
        "search_results": [],
        "extracted_content": [],
        "synthesized_answer": "",
        "final_answer": "",
        "sources": [],
        "current_step": "initialized"
    })

    print(f"ğŸš€ Starting research for: {question}")
    print("=" * 50)

    # Run the graph
    final_state = None
    try:
        for step in research_assistant.stream(initial_state):
            for node_name, node_output in step.items():
                print(f"â†’ {node_name}: {node_output['current_step']}")
            final_state = node_output

        print("=" * 50)
        print("âœ… Research completed!")
        return final_state

    except Exception as e:
        print(f"âŒ Error in research flow: {e}")
        return initial_state

def display_results(final_state: Dict[str, Any]):
    """Display the results in a formatted way"""

    print("\n" + "ğŸ”" * 20)
    print("FINAL RESULTS")
    print("ğŸ”" * 20)

    print(f"\nğŸ“ QUESTION: {final_state['question']}")

    print(f"\nğŸ’¡ ANSWER:")
    print("-" * 50)
    print(final_state['final_answer'])
    print("-" * 50)

    if final_state['sources']:
        print(f"\nğŸ“š SOURCES ({len(final_state['sources'])}):")
        for i, source in enumerate(final_state['sources'], 1):
            print(f"\n[{i}] {source['title']}")
            print(f"    ğŸ“ URL: {source['url']}")
            print(f"    ğŸ“„ Snippet: {source['snippet'][:100]}...")
    else:
        print("\nğŸ“š No sources available")

    print(f"\nğŸ“Š STATS:")
    print(f"   â€¢ Search results: {len(final_state['search_results'])}")
    print(f"   â€¢ Sources extracted: {len(final_state['extracted_content'])}")
    print(f"   â€¢ Answer length: {len(final_state['final_answer'])} characters")

# Test the research assistant

# Test with a simple question first
#test_question = "Explain mHC: Manifold-Constrained Hyper-Connections by DeepSeek"
test_question = "Tell me who is Nagesh Singh Chauhan"

print("ğŸ§ª Testing Mini Perplexity with LangGraph...")
print(f"Question: {test_question}")

try:
    final_state = run_research_assistant(test_question)
    display_results(final_state)
except Exception as e:
    print(f"âŒ Main test failed: {e}")

    # Fallback test
    print("\nğŸ”„ Trying fallback test...")
    simple_question = "What is machine learning?"
    final_state = run_research_assistant(simple_question)
    display_results(final_state)

